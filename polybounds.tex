% $Id: polyarith.tex,v 1.1 1992/05/10 19:43:45 rz Exp rz $
\chapter{Bounds on Polynomials}
\label{PBounds:Chap}


A number of the more sophisticated polynomial algorithms need
estimates on the size of the coefficients of factors of a polynomial.
The most natural of these results give bounds on the size of linear
factors of univariate polynomials, \ie, bounds on the size of zeroes
of univariate polynomials.  These bounds are very useful when
computing the polynomial zeroes numerically.  The linear factor
estimates can be extended to give bounds on the size of larger degree
factors and factors of multivariate polynomials.  These estimates are
used in calculations of the GCD's and factorizations of polynomials
over the integers.

\sectref{PB:Heights:Sec} makes precise how to estimate the size of a
polynomial, taking into account the size of the coefficients.  These
different ways of measuring the size divide into two different
classes, those that give equal weight to all of the coefficients and
those that give different weights to coefficients of terms of
differing degrees.  In \sectref{Uniform:Bounds:Sec} we derive bounds
on the size of the coefficients based on unweighted, or uniform, size
measures.  \sectref{Weighted:Bounds:Sec} presents some bounds that
make use of weighted coefficients size measures.  

The sharper bounds than those of \sectref{Uniform:Bounds:Sec} can be
derived for the special case of linear factors, \ie, for zeroes of a
polynomial.  These results are given in \sectref{PB:RootSize:Sec}.
Finally, in \sectref{PB:RootSeparate:Sec} we give lower bounds on
distance between two roots of a polynomial, \ie, $|\alpha_i - \alpha_j|$.

\section{Heights of Polynomials}
\label{PB:Heights:Sec}

To make this precise we first define a few different measures on the
coefficients of $P(X)$, 
\begin{equation}\label{PA:p:Def:Eq}
P(X) = p_0 X^d + \cdots + p_{d-1} X + p_d,
\end{equation}
where $p_0 \not= 0$.

We define the {\em height}\index{height, of a polynomial} of a $P(X)$ to be 
\[
\|P \|_{\infty} = 
  \max \{\left|p_{0}\right|, \left|p_{1}\right|, \ldots, \left|p_{d}\right|\}.
\]
In the notation of \chapref{Lattice:Chap} this is the $\|\,\|_{\infty}$ norm
of the vector $\tuple{p_{0},\ldots, p_{d}}$.  Similarly, we define the
$\|\,\|_{2}$ norm of a polynomial by
\[
\|P \|_{2} = \left(\left|p_{0}\right|^{2} + \cdots +
    \left|p_{d}\right|^{2}\right)^{1/2}.
\]
In the literature, some authors use $|P|$ for $\|P\|_{2}$, while others use
$|P|$ to mean $\|P\|_{\infty}$.  The first notation is common in the
computational complexity community, while the second is more common in
diophantine analysis.  In keeping with the notation used in
\chapref{Lattice:Chap}, and thus more in keeping with diophantine analysis
community, we will use $|P|$ for $\|P\|_{\infty}$ and $\|P\|$ for
$\|P\|_{2}$, when $P$ is a polynomial.  The relationship between these
bounds is given by the following proposition.

\begin{proposition}\label{PB:HeightRel:Prop}
Let $P$ be a univariate polynomial of degree $d$ over $\C$.  Then
\[
|P| \le \|P\| \le \sqrt{d+1} |P|.
\]
\end{proposition}

\begin{proof}
$|P|$ is equal to $\|P\|$ if only if all but one of the coefficients
of $P$ are zero.  Otherwise $|P| < \|P\|$.  Let $p_{\ell}$ be the
coefficient of $P$ that has the largest absolute value. Then
\[
\|P\|^2 = |p_0|^2 + |p_1|^2 + \cdots + |p_d|^2 \le (d+1) |p_{\ell}|^2.
\]
\end{proof}

When $P(X) = F(X) G(X)$, one might expect the height of $F$ to be less
than the height of $P$.  This is obviously true if $F$, $G$ and $P$
all have positive coefficients; however, it is not true in general.
The canonical example along these lines is the factorization of
$X^{n}-1$ over the rational integers.  Though it is plausible that the
coefficients of the factors of $X^{n}-1$ all be $\pm1$, in fact they
can grow rather quickly. The smallest example for which $f$ has height
greater than $1$ is $n=105$.  $X^{105}-1$ has an irreducible factor of
degree $47$ that has two terms whose coefficients are $-2$.  {\Erdos}
has shown \cite{Erdos46}
\begin{proposition} [\Erdos]
Let $A_n$ denote the absolute value of the largest coefficient in an
irreducible factor of $X^n -1$.  For infinitely many $n$
\[
A_n > e^{c (\log n)^{4/3}}
\]
for some positive coefficient $c$.  In particular we can take $n = 2
\cdot 3 \cdot 5 \cdots p_k$.
\end{proposition}

Another useful measure of a polynomial's size is how far its zeroes are
outside of the unit disk.  Denote the zeroes of $P(X)$ by $\alpha_1,
\ldots, \alpha_d$.  We define $M(P)$ to be
\[
M(P) = |p_0| \prod_{1\le i \le d} \max\{1, |\alpha_{i}|\},
\]
which is the product of the absolute values of the zeroes of $p(x)$ that
lie outside the unit disk.

All of the polynomial norms discussed thus far treat each of the
coefficients equally.  However, when multiplying polynomials the
``middle'' coefficients tend to get larger than the leading and
trailing coefficients, \eg{}
\[
(X+1)^5 = X^5 + 5 X^4 + 10 X^3 + 10 X^2 + 5 X +1.
\]
A norm that takes advantage of this behavior was introduced by
{\Beauzamy} \etal{} \cite{Beauzamy90}.  The {\em weighted $L_2$ norm} of
$P(x)$ is defined to be
\[
[P(X)]_2 = \left(\sum_{i=0}^d {d \choose i}^{-1} |p_i|^2\right)^{1/2}.
\]

\section{Uniform Coefficient Bounds}
\label{Uniform:Bounds:Sec}

The goal of this section is to establish unweighted bounds on the size
of the coefficients in a factor of a polynomial.  The most natural
norm to use for this purpose is the height of the polynomial, $|P|$,
although a bound based on the $2$-norm would also be useful since,
\[
\|P\|_2 \ge |P|.
\]
However, for dealing with factors of polynomials, the $M$ norm is far
easier to work with, since $M(PQ) = M(P) \cdot M(Q)$.  Unfortunately,
$M(P)$ is rather difficult to compute from $P$ and given $M(P)$ it is
not clear what one can say about $|P|$, which is what is needed for
many applications.  Our first step is thus to relate $M(P)$ to
$\|P\|$.

We begin by developing a different form for $M(P)$ that is more useful
for theoretical calculations, but less useful for numerical
computations.  One the tools we need to derive this different form is
{\Jensen}'s formula\index{Jensen's formula} \cite{Jensen:Formula}.

\begin{proposition}[Jensen]
\label{Jensen:Formula:Prop}
Let $f(z)$ be a meromorphic function on the unit disk that does not vanish
at the origin or on the boundary of the disk.  Denote the zeroes of $f(z)$
which fall within the unit disk by $\alpha_{1}, \ldots, \alpha_{m}$ and the
poles by $\beta_{1}, \ldots, \beta_{n}$.  Then
\begin{equation}
\label{Jensen:Formula:Eq}
\begin{eqalign}
\int_{0}^{1} \log \left|f(e^{2\pi i t})\right|\,dt = 
\log |f(0)| &+ \log \frac{1}{|\alpha_{1}|} + \cdots + 
                \log \frac{1}{|\alpha_{m}|} \\
       & + \log |\beta_{1}| + \cdots + \log |\beta_{n}|
\end{eqalign}
\end{equation}
\end{proposition}

If $f(z) = P(z)$ is a polynomial and thus has only zeroes, we can
write
\eqnref{Jensen:Formula:Eq} as
\begin{equation}
\label{Jensen:Formula:a:Eq}
\exp \int_{0}^{1} \log \left| P(e^{2\pi i t})\right| \, dt =
\frac{|P(0)|}{\left| \alpha_{1} \cdots \alpha_{m}\right|},
\end{equation}
where $\alpha_{1}, \ldots, \alpha_{m}$ are the zeroes of $P(z)$ that lie
within the unit circle.  We can write the product of all of the zeroes
of $P(z)$ as
\[
\alpha_1 \alpha_2 \cdots \alpha_d = \frac{p_d}{p_0} =
\frac{P(0)}{p_0}.
\]
Consequently, we can write $M(P)$ as
\[
M(P) = \exp \int_{0}^{1} \log \left| P(e^{2\pi i t})\right| \, dt.
\]

The following proposition is a continuous analog of the fact that the
geometric mean of a sequence of positive numbers is less than the
arithmetic mean, unless all the elements of the sequence are equal.
In this form it is due to {\Jensen} \cite{Jensen06}.

\begin{proposition}[Jensen]
\label{Jensens:Inequality:Prop}
If $f(x)$ is non-constant, Riemann integrable function then
\[
\exp \left\{ \int_0^1 \log f(x)\, dx\right\} < \int_0^1 f(x) \, dx
\]
\end{proposition}

\begin{proof}
The arithmetic mean of a sequence of numbers $a_1, \ldots, a_n$ is
\[
\frac{a_1 + a_2 + \cdots + a_n}{n}
\]
while the geometric mean is defined to be
\[
\sqrt[n]{a_1 a_2 \cdots a_n}.
\]
For $n = 2$ we have
\[
a_1 a_2 = \left(\frac{a_1 + a_2}{2}\right)^2 
     + \left(\frac{a_1 - a_2}{2}\right)^2
        < \left(\frac{a_1 + a_2}{2}\right)^2,
\]
unless $a_1 = a_2$.  Applying this recursively, we have
\[
\left(\frac{a_1+a_2}{2}\right) \left(\frac{a_3+a_4}{2}\right) \le 
\frac{1}{2} \left(\left(\frac{a_1+a_2}{2}\right)+
\left(\frac{a_3+a_4}{2}\right) \right).
\]
So, we have
\[
a_1 a_2 a_3 a_4 \le \left(\frac{a_1+a_2+a_3+a_4}{4}\right)^4.
\]
And by induction if $N = 2^k$,
\[
a_1 a_2 \cdots a_{N} \le \left(\frac{a_1+a_2+\cdots+a_N}{N}\right)^N.
\]

Now consider the sum
\[
\begin{eqalign}
  \exp \left\{\sum_{i=1}^N \frac{1}{N}\log f(\frac{i}{N})\right\}
  &= \left(f(\frac{1}{N}) f(\frac{2}{N}) \cdots
f(\frac{N}{N})\right)^{1/N} \\
& \le \left(\frac{(f(\frac{1}{N}) + f(\frac{2}{N}) + \cdots +
f(\frac{N}{N})}{N}\right).
\end{eqalign}
\]
Taking the limit as $N$ goes to infinity gives the proposition.
\end{proof}

The following proposition of {\LandauE} \cite{Landau:Zeroes} shows
that $M(P)$ is bounded by $\| P\|_{2}$.  Its proof and
\propref{Uni:Coef:MP:Prop} are ascribed by {\Lang}
\cite{Lang:Diophantine:Geometry} to {\Waldschmidt}.

\begin{proposition}[Landau]
\label{Landau:Zeroes:Prop}
Let $P(X)$ be a univariate polynomial over $\C$, then $M(P) \le
\|P\|$.
\end{proposition}

\begin{proof}
Writing the $|P(X)|^{2}$ (absolute value) as $P(X) \overline{P(X)}$ we have
\[
\begin{eqalign}
  \int_{0}^{1} \left| P(e^{2\pi i t})\right|^{2} \,dt
     & = \int_{0}^{1} P(e^{2\pi i t})\,P(e^{-2\pi i t}) \, dt\\
 & = \int_{0}^{1} 
       \sum_{\scriptstyle 0\le r \le d \atop \scriptstyle 0 \le s \le d} 
          P_{r} \overline{P_{s}} e^{2\pi i (r-s) t}
       \,dt\\
 & = \int_{0}^{1} \sum_{0 \le r \le d} \left| a_{r} \right|^{2} \, dt 
 = \| P(X) \|^{2}.
\end{eqalign}
\]
So
\[
\|P\|_{2} 
 = \left( \int_{0}^{1} \left| P(e^{2\pi i t})\right|^{2} \,dt \right)^{1/2}.
\]
By \propref{Jensens:Inequality:Prop} we have
\[
M(P)^2 = \exp \int_{0}^{1} 2\log \left| P(e^{2\pi i t})\right| \, dt \le
\int_{0}^{1} \left| P(e^{2\pi i t})\right|^{2} \,dt
 = \|P\|_2^2.
\]
From which the proposition follows immediately.
\end{proof}

\begin{proposition} \label{Uni:Coef:MP:Prop}
Let $P(X)$ be a polynomial in $\C[X]$ of degree $d$, then
\[
2^{-d} |P| \le M(P) \le \sqrt{d+1} |P|.
\]
\end{proposition}

\begin{proof}
The upper bound is a combination of Propositions~\ref{Landau:Zeroes:Prop} and
\ref{PB:HeightRel:Prop}. 

As before, let $\alpha_{i}$ be the zeroes of $P(X)$, then
\[
\begin {eqalign}
P(x) & = p_0 (X - \alpha_{1}) \cdots (X - \alpha_{d}) \\
 & = p_0 X^{d} - p_0(\alpha_{1} + \cdots + \alpha_{d}) X^{d-1} + 
\cdots + (-1)^{d} p_0 \alpha_{1} \cdots \alpha_{d}.
\end{eqalign}
\]
$(-1) ^{i} p_{i}$ is the sum of all products of $i$ distinct
$\alpha_{i}$'s.  There are $d \choose i$ distinct products in each sum
and each is no larger than $M(P)$ so
\begin{equation}\label{CoefZ:Bound:Eq}
\left| p_{i} \right| \le {d \choose i} M(P)
 \le 2^{d} M(P).
\end{equation}
This is the lower bound.
\end{proof}

Notice that inequality \eqnref{CoefZ:Bound:Eq} is valid for $i = 0$.
The first half of this inequality is quite useful and will be used
again.

\medskip
The following proposition, and its multivariate extension are the key
results we need to estimate the size of the factors of a polynomial.

\begin{proposition} [{\Gelfond}] \label{Factor:CBound:Prop}
Let $f_{1}$ and $f_{2}$ be two monic polynomials in $\C[x]$ such that
$\deg f_1 + \deg f_2 = d$, then
\[
\left|f_{1}\right|\, \left| f_{2}\right| 
  \le (d+1)^{1/2}2^{d}\left| f_{1} f_{2} \right|.
\]
\end{proposition}

\begin{proof}
We begin with the identity $M(f_{1}) M(f_{2}) = M(f_{1} f_{2})$ and
estimate each side using \propref{Uni:Coef:MP:Prop}.  Applying the upper
bound of \propref{Uni:Coef:MP:Prop} we have
\[
2^{-d}\left|f_{1}\right| \,\left|f_{2}\right| \le M(f_{1})M(f_{2}).
\]
The lower bound gives
\[
M(f_{1} f_{2}) \le (d+1)^{1/2} \left| f_{1} f_{2}\right|,
\]
which combined with $M(f_{1}) M(f_{2}) = M(f_{1} f_{2})$ gives the
proposition.
\end{proof}

\subsection{Uniform Multivariate Bounds}

\propref{Factor:CBound:Prop} can be generalized to $n$ variables by defining
\[
M(p(x_{1}, \ldots, x_{n})) = \exp \int_{0}^{1}\cdots \int_{0}^{1} \log
\left|p(e^{2\pi i t_{1}}, \ldots, e^{2\pi i t_{n}})\right| \, dt_{1}
\cdots dt_{n}.
\]
The $\|\,\|$ norm of a multivariate polynomial is defined in the same was
as in the univariate case, but it also admits an integral representation:
\[
\|p(x_{1}, \ldots, x_{n})\|_2 = 
  \left(\int_{0}^{1}\cdots \int_{0}^{1}
    \left|p(e^{2\pi i t_{1}}, \ldots, e^{2\pi i t_{n}})\right|^2
  \, dt_{1} \cdots dt_{n}\right)^{\frac{1}{2}}.
\]

\subsection{Graeffe Methods}
\label{Graeffe:Bound:Sec}

An alternative approach to bounding $M(P)$ is contained in the
following proposition presented in \cite{Cerlienco87}.  We begin by
defining some interesting polynomials related to $P(X)$.  Define
\[
P_k(X) = p_0^k \prod_i \left(X - \alpha_i^k\right),
\]
where $\alpha_i$ are the zeroes of $P(X)$.  The $P_k(X)$ are
relatively easy to compute from the $P(X)$.  In particular, if
\[
P(X) = P_e(X^2) + X P_o(X^2),
\]
then 
\[
P_2(X) = P_e(X)^2 - X P_o(X)^2.
\]
By repeating this process we can get $P_4(X)$, $P_8(X)$, etc.  Other
approaches are discussed in \exeref{PowerZero:Ex}.

\begin{proposition}  
\label{Graeffe:Bound:Prop}
Let $P(X)$ be a polynomial, and define
$P_k(X)$ as above.  Then
\[
2^{-d} \|P_k\| \le M(P)^k \le \|P_k\|
\]
\end{proposition}

\begin{proof}
Since the zeroes of $P_k$ are the $k${\th} powers of zeroes of $P$, we
have $M(P_k) = M(P)^k$.  By \propref{Landau:Zeroes:Prop}, we have
\[
M(P)^k =  M(P_k) \le \|P_k\|, 
\]
which is the upper bound.  The lower bound is only slightly more
tricky.  We have
\[
\|P\|^2 = p_0^2 + p_1^2 + \cdots + p_d^2 
   \le (|p_0|^2 + |p_1| + \cdots |p_d|)^2.
\]
By \eqnref{CoefZ:Bound:Eq} we have
\[
\begin{eqalign}
\|P\|^2 & \le \left({d \choose 0} M(P) + {d \choose 1} M(P) + \cdots
+ {d \choose d} M(P)\right)^2, \\
  & \le 2^{2d} M(P)^2.
\end{eqalign}
\]
Taking square roots and replacing $P$ by $P_k$ in this inequality
gives
\[
\|P_k\| \le 2^d M(P)^k = 2^d M(P_k).
\]
\end{proof}

\begin{figure}
\begin{center}
\epsfxsize=3.25in\epsfbox{Pix/GraeffeB.eps}
\end{center}
\caption{Upper and lower bounds on $M(P)$\label{Graeffe:Bound:Fig}}
\end{figure}

The lower bound of \propref{Graeffe:Bound:Prop} is not particularly
sharp, but the upper bound can be made quite sharp, by increasing the
size of $k$.  

To illustrate this technique consider the following polynomial
suggested by \cite{Cerlienco87}
\[
P(X)  = X^6 + X^5 + 6X^4 - 5X^3 + 3X^2 + 2,
\]
which has zeroes
\[
-0.9015 \pm 2.4962 i, \quad0.6745 + \pm 0.5646i, \quad
-0.2730 \pm 0.5408i.
\]
Only the first two zeroes are outside the unit circle and their
product is $7.0436280 = M(P)$.  \figref{Graeffe:Bound:Fig} shows the
upper and lower bounds on $M(P)$ based on $P_k$ as $k$ ranges from $1$
to $32$.  For $k = 11$ we already get the quite good upper bound of
$7.0470$, while the lower bound is $4.8284$.  For $k = 2048$, the
lower bound rises to $7.02934$, but computationally this is quite
expensive, due to the size of the coefficients of $P_m(X)$.

\section{Weighted Coefficient Bounds}
\label{Weighted:Bounds:Sec}


The relationship between $|p_i|$ and $M(P)$ given in
\eqnref{CoefZ:Bound:Eq} can be used to derive a bound on the
individual coefficients of divisors of a polynomial, as noted by
Mignotte \cite{Mignotte74}.  

\begin{proposition}[Mignotte]
Let $Q(X) = q_0 X^{r} + q_{1} X^{r-1} + \cdots + q_{r}$ be a
polynomial over $\C$ that divides $P(X)$.  Then
\[
\left| q_{i} \right| \le {r \choose i} \left\|P\right\|.
\]
\end{proposition}
\begin{proof}
By \eqnref{CoefZ:Bound:Eq} we have,
\[
|q_i| \le {r \choose i} M(Q).
\]
Since every zero of $Q$ is a zero of $P$, we have $M(Q) \le M(P)$, and
by \propref{Landau:Zeroes:Prop}, $M(P) \le \|P\|$, which gives the
proposition. 
\end{proof}

This proposition indicates that the coefficients closer to the middle
of $q(x)$ can be substantially larger than coefficients near the
leading and trailing terms.  Recently, this observation was made
substantially more precise by using the {\em weighted $L_2$ norm}.

The following proposition was proven in \cite{Beauzamy90}

\begin{proposition}
Let $f$ and $g$ be polynomials in one variable of degree $m$ and $n$
respectively.  Then
\[
[fg]_2 \ge \sqrt{\frac{m! \, n!}{(m+n)!}} [f]_2 \, [g]_2,
\]
and this result is best possible.
\end{proposition}

\begin{proof}
{\em Provide a proof}
\end{proof}

The following proposition are copied from \cite{Beauzamy92a}.

\begin{proposition} 
Let $p(x)$ be a univariate polynomial in $x$ over $\Z$, with
non-vanishing constant term.  Let $q(x)$ be a divisor of $p(x)$, also
with rational integer coefficients.  Denote $\deg p$ by $d$, and $m =
\deg q$.  The each coefficient $q_i$ of
$q(x)$ satisfies
\[
|q_i| \le \sqrt{\frac{1}{2} {m \choose i} {d \choose m}} [p]_2
   \le \sqrt{\frac{n!}{2(d-m)!\, (m-i)!\, i!}} [p]_2.
\]
Furthermore, 
\[
|q| \le \frac{3^{3/4}}{2\sqrt{\pi}}\, \frac{3^{d/2}}{\sqrt{d}} [p]_2.
\]
\end{proposition}


\section{Size of Zeroes of a Polynomial}
\label{PB:RootSize:Sec}

The following inequality is due to {\Cauchy} \cite{Cauchy:Exercises}.

\begin{proposition}[Cauchy]
\label{Cauchy:Zero:Bound:Prop}
Let $p(x) = x^{d} + p_{1} x^{d-1} + \cdots + p_{d}$ be a non-constant,
monic polynomial with coefficients in $\C$.  Then a root of $p(x)$
$\alpha$, satisfies the inequality
\begin{equation}
\label{Cauchy:Zero:Ineq:Eq}
\left|\alpha\right| < 1 + \max \{1, |p_{1}|, \ldots, |p_{n}|\} 
  = 1 + \left| p \right|.
\end{equation}
\end{proposition}

\begin{proof}
Assume $\alpha$ is greater than $1$, otherwise the proposition is obvious.
Rewriting \eqnref{Cauchy:Zero:Ineq:Eq} and taking absolute values we have
\[
\left| \alpha \right|^{d} = \left| p_{1} \alpha^{d-1} + \cdots + p_{n} \right|
 \le \left| \alpha^{d-1} + \cdots + 1 \right| | p| 
 \le { |\alpha|^{d} \over |\alpha| - 1} |p|,
\]
which implies $|\alpha| \le 1 + |p|$.
\end{proof}

Thus far we have assumed that $p(x)$ is monic.  If this is not the case, we
can monicize the polynomial and apply \propref{Cauchy:Zero:Bound:Prop}.  In
particular, let 
\[
f(x) = a_{0} x^{d} + \cdots + a_{d},
\]
and let $\alpha$ be a zero of $f(x)$.  Applying the proposition to
\[
x^{d} + \frac{a_{1}}{a_{0}}x^{d-1} + \cdots + \frac{a_{d}}{a_{0}}
\]
we have
\[
\begin{eqalign}
|\alpha| & 
   \le 1 + \max\{ \frac{a_{1}}{a_{0}}, \ldots, \frac{a_{d}}{a_{0}} \},\\
 & \le \frac{\left|a_{0}\right| + 
               \max \{\left|a_{1}\right|, \ldots, \left|a_{d}\right|
\}}{\left|a_{0}\right|}.
\end{eqalign}
\]
Applying \propref{Cauchy:Zero:Bound:Prop} to the monic polynomial of
$1/\alpha$:
\[
x^{d} + \frac{a_{d-1}}{a_{d}} x^{d-1} + \cdots + \frac{a_{0}}{a_{d}}
\]
gives
\[
|\alpha| \ge \ \frac{\left|a_{d}\right|}{\left|a_{d}\right| + 
   \max\{ \left|a_{0}\right|, \ldots, \left|a_{d-1}\right|\}}.
\]
Putting these two results together we have for each zero of $f(x)$,
\[
\frac{\left|a_{0}\right| + 
               \max \{\left|a_{1}\right|, \ldots, \left|a_{d}\right|
\}}{\left|a_{0}\right|}
\ge
|\alpha| \ge \ \frac{\left|a_{d}\right|}{\left|a_{d}\right| + 
   \max\{ \left|a_{0}\right|, \ldots, \left|a_{d-1}\right|\}}.
\]

\section{Discriminants and Zero Separations}
\label{PB:RootSeparate:Sec}

This section obtains bounds on the difference between zeroes of a
polynomial $P(X)$.  One of the most useful expressions involving the
difference of zeros of $P(X)$ is the {\em discriminant} of $P$, which
is, essentially, the square of the product of differences of pairs of
zeroes of $P(X)$.  This quantity is quite useful in the study of
algebraic extensions.

The quantity of interest is a lower bound on the smallest difference
between two distinct zeros of a polynomial.  This quantity is
essential for numerical calculations of the zeroes of a polynomial.

\paragraph{Discriminants}

As usual denote the zeroes of $P(X)$ by $\alpha_1, \ldots, \alpha_d$
and consider the matrix
\[
P_D = 
\left(\begin{array}{ccccc}
1 & \alpha_1 & \alpha_1^2 & \cdots & \alpha_1^{d-1} \\
1 & \alpha_2 & \alpha_2^2 & \cdots & \alpha_2^{d-1} \\
\vdots & & \vdots & \cdots & \vdots \\
1 & \alpha_d & \alpha_d^2 & \cdots & \alpha_d^{d-1} 
\end{array}\right).
\]
This is a \keyi{Vandermonde matrix} and is discussed in some detail in
\sectref{Vandermonde:Sec}.  Its determinant is equal to the product of
the difference of the zeroes of $P(X)$:
\[
\det|P_D| = \prod_{1\le i < j \le d} ( \alpha_i - \alpha_j).
\]
The {\em discriminant} of $P(X)$\index{discriminant!of a
polynomial|bold} is defined to be
\begin{equation}\label{PB:Discriminant:Eq}
D(P) = p_0^{2d-2} \det|P_D|^2.
\end{equation}

The discriminant of a polynomial can most effectively computed using
resultants.  We can write $P(X)$ as
\[
P(X) = p_0 (X - \alpha_1) (X - \alpha_2) \cdots (X - \alpha_d).
\]
Its derivative is
\[
P'(X) = p_0(X - \alpha_2) \cdots (X - \alpha_d) + \cdots + p_0(X - \alpha_1) \cdots (X -
\alpha_{d-1}).
\]
Evaluating at $\alpha_i$ we have
\[
P'(\alpha_i) = p_0(\alpha_i - \alpha_1) \cdots \widehat{(\alpha_i -
\alpha_i)} \cdots (\alpha_i - \alpha_d).
\]
Taking the product over all zeroes of $P(X)$ gives
\[
\begin{eqalign}
P'(\alpha_1) \cdots P'(\alpha_d) 
  & \displaystyle
    = (-1)^{d(d-1)/2} p_0^d \prod_{1\le i< j \le n}(\alpha_i - \alpha_j)^2,\\
  & \displaystyle
    = (-1)^{d(d-1)/2} p_0^{2-d} D(P).
\end{eqalign}
\]
By the definition of the resultant \eqnref{Resultant:Def:Eq}
\[
\res_X (P(X), P'(X)) = (-1)^{d(d-1)} p_0^{d-1}P'(\alpha_1) \cdots
P'(\alpha_d).
\]
Since either $d$ or $d-1$ is even, we have
\[
D(P) = (-1)^{d(d-1)/2} \frac{\res_X (P(X), P'(X))}{p_0}.
\]

Using \propref{Resultant:Bound:Prop} we can bound $|D(P)|$ as
\begin{equation}\label{PB:DiscrmBound1:Eq}
|D(P)| \le (\deg P + \deg P')! |P|^{d-1} |P'|^d \le (2d-1)! d^d
|P|^{2d-1}.
\end{equation}


We can sharpen this bound by apply the Hadamard inequality
(\longpropref{Hadamard:Ineq:Prop}) to $P_D$.  First assume that all of
the zeroes of $P(X)$ have absolute value less than $1$.  For each row
of $P_d$, we have
\[
\|\langle 1, \alpha_i, \ldots \alpha_i^{d-1} \rangle\|_2
\le \sqrt{1^2 + 1^2 + \cdots + 1^2} \le \sqrt{d}.
\]
So $\det|P_D| \le d^{d/2}$, and $D(P) \le d^d p_0^{2-2d}$.

More generally, assume that the zeroes of $P(X)$ satisfy
\[
|\alpha_1| \ge |\alpha_2| \ge \cdots \ge |\alpha_m| \ge 1 \ge |\alpha_{m+1}|
\ge \cdots \ge |\alpha_d|.
\]
So, the first $m$ zeroes of $P(X)$ lie outside the unit disk.
Dividing the first row by $\alpha_1^{d-1}$, the second row by
$\alpha_2^{d-1}$ and so on, we have
\[
Q_D = \frac{P_d}{(\alpha_1 \cdots \alpha_m)^{d-1}} =
\left(\begin{array}{ccccccc}
\alpha_1^{1-d} & \alpha_1^{2-d} & \alpha_1^{3-d} & \cdots & 1 \\
\vdots & & \vdots & \cdots & \vdots \\
\alpha_m^{1-d} & \alpha_m^{2-d} & \alpha_m^{3-d} & \cdots & 1 \\
1 & \alpha_{m+1} & \alpha_{m+1}^2 & \cdots & \alpha_{m+1}^{d-1} \\
\vdots & & \vdots & \cdots & \vdots \\
1 & \alpha_d & \alpha_d^2 & \cdots & \alpha_d^{d-1} 
\end{array}\right).
\]
Each element of $Q_D$ is has absolute value less than $1$ so by
Hadamard's inequality, $\det|Q|$ can also be bounded by $d^{d/2}$.
Thus,
\[
\det|P|= (\alpha_1 \cdots \alpha_m)^{d-1} \det|Q_D| \le 
\frac{M(P)^{d-1}}{p_0^{d-1}} d^{d/2},
\]
in general.  This immediately gives the following upper bound for the
discriminant. 

\begin{proposition}
If $P(X)$ is a univariate polynomial over $\C$ of degree $d$ and
leading coefficient $p_0$ then the absolute value of the discriminant
of $P(X)$ is bounded by
\[
|D(P)| \le d^d M(P)^{2(d-1)} \le d^d \|P\|^{2(d-1)}.
\]
\end{proposition}

The last inequality follows from \propref{Landau:Zeroes:Prop}.  Lower
bounds for the discriminant are much more subtle.  We quote the
following result of {\Odlyzko} in this regard \cite{Odlyzko75,Odlyzko77}

\begin{proposition}
Let $P(X)$ be a univariate, square free polynomial over $\Z$ of degree
$d$.  Denote the number of real zeroes of $P(X)$ by $r_1$ and the
number of complex zeroes by $2r_2$.  Then
\[
\begin{eqalign}
|D(P)| & \ge (60.1)^{r_1} (22.2)^{2r_2} e^{-254}, \\
|D(P)| & \ge (58.6)^{r_1} (21.8)^{2r_2} e^{-70}. \\
\end{eqalign}
\]
Assuming the generalized Riemann hypothesis
\[
|D(P)| \ge (188.3)^{r_1} (41.6)^{2r_2} e^{-3.7\times10^8}.
\]
\end{proposition}


\paragraph{Zero Separation}

In order to numerically determine the zeroes of a univariate
polynomial, $P(X)$, it is important to know when two zeroes are
identical.  Since we cannot have exact finite numerical
representations of the zeroes of $P(X)$, we need a lower bound on how
close two distinct zeroes of a $P(X)$ can be.  We define the {\em zero
separation}\index{zero separation|bold} of $P$ to be
\[
\Delta(P) = \min_{i \not= j} \left| \alpha_{i} - \alpha_{j} \right|.
\]
\addsymbol{$\Delta(P)$}{minimum zero separation of two distinct zeroes
of the univariate polynomial $P$}

Assume that $|\alpha_i - \alpha_j|$ is the difference of roots of
$P(X)$, $i < j$.  We will use a slight variation of the techniques
used for discriminants to get an upper bound on $\det|P_D/(\alpha_i -
\alpha_j)|$.  This immediately yields a lower bound on $\Delta(P)$. 

Start with $P_D$ and subtract the $j${\th} row from the $i${\th} row,
\[
\left(\begin{array}{ccccccc}
1 & \alpha_1 & \alpha_1^2 & \cdots & \alpha_1^{d-1} \\
\vdots & & \vdots & \cdots & \vdots \\
0 & \alpha_i - \alpha_j & \alpha_i^2 - \alpha_j^2 & \cdots &
\alpha_i^{d-1} - \alpha_j^{d-1} \\
\vdots & & \vdots & \cdots & \vdots \\
1 & \alpha_d & \alpha_d^2 & \cdots & \alpha_d^{d-1} 
\end{array}\right).
\]
$\alpha_i - \alpha_j$ divides the $i{\th}$ row of this matrix so
\[
\frac{\det|P_D|}{\alpha_i - \alpha_j} = 
\left(\begin{array}{ccccccc}
1 & \alpha_1 & \alpha_1^2 & \cdots & \alpha_1^{d-1} \\
\vdots & & \vdots & \cdots & \vdots \\
0 & 1 & q_2 & \cdots & q_{d-1} \\
\vdots & & \vdots & \cdots & \vdots \\
1 & \alpha_d & \alpha_d^2 & \cdots & \alpha_d^{d-1} 
\end{array}\right),
\]
where 
\[
q_{\ell} = \alpha_i^{\ell-1} + \alpha_i^{\ell -2} \alpha_j + \cdots +
\alpha_j^{\ell-1}.
\]
We now divide the first $m$ rows by $\alpha_{\ell}^{d-1}$ as before.
The elements of all but the $i${\th} row have absolute value no
greater than $1$.  The absolute values of the elements of the $i${\th}
are bounded by $0, 1, 2, \ldots, {d-1}$ respectively, since
$|\alpha_i| > |\alpha_j|$.  

Applying Hadamard's inequality gives
\[
\frac{\det|P_D|}{(\alpha_i - \alpha_j) (\alpha_1 \ldots
\alpha_m)^{d-1}} 
\le d^{(d-1)/2}\sqrt{0^2 + 1^2 + \cdots + (d-1)^2}.
\]
The sum of squares inside the square root is easily bounded:
\[
0^2 + 1^2 + \cdots + (d-1)^2 = \frac{m(m-1)(2m-1)}{6} \le
\frac{m^3}{3}.
\]
Thus,
\[
\frac{p_0^{d-1} \det|P_D| \sqrt{3}}{M(P)^{d-1} d^{(d+2)/2}} 
   < |\alpha_i - \alpha_j|.
\]
Applying \eqnref{PB:Discriminant:Eq} gives {\Mahler}'s bound on the separation of the roots of $P(X)$ \cite{Mahler64}.

\begin{proposition}[{\Mahler}]
Let $p(x)$ be a square free polynomial of degree $d$ with discriminant $D$.
Then
\[
\Delta(p) > \sqrt{\frac{3 |D|}{d^{d+2}}} M(p)^{1-d}
\]
\end{proposition}

Using \propref{Landau:Zeroes:Prop} we have
\[
\Delta(p) > \sqrt{\frac{3 |D|}{d^{d+2}}} \|p\|^{1-d}.
\]

\section*{Notes}

\footnotesize

Emma {\LehmerE} \cite{LehmerE36} surveyed the early work on the size the
coefficients in cyclotomic polynomials.  {\Apostol} \cite{Apostol75}
contains a good bibliography to more recent results.

{\em For bounds on size of coefficients see
\cite{Kannan:Lenstra:Lovasz,Mignotte82,Mignotte81} }

\notesectref{Uniform:Bounds:Sec}  A nice proof of
\propref{Jensen:Formula:Prop} is contained in problems 120 and 175,
part III of P\'olya and Szeg\"o \cite{Polya:Szego}.

{\Gelfond}'s proof of \propref{Factor:CBound:Prop}
\cite{Gelfond:Transcendence} (pages 135--139) uses only elementary
techniques but it is quite a bit more lengthy than the one provided
here.

\notesectref{Graeffe:Bound:Sec}
Alternative approaches to computing $M(P)$, which are more efficient,
are discussed in \cite{Cerlienco87}.  One interesting observation that
can be made from \figref{Graeffe:Bound:Fig} is that the
$\|P_k\|^{1/k}$ tends to be an especially good bound on $M(P)$ when
$k$ is a prime number.  Can this statement be made more precise and
generalized? 

\notesectref{PB:RootSeparate:Sec} The presentation here follows that
of Mahler's original paper \cite{Mahler64}.

\normalsize
