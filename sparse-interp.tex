%$Id: interp.tex,v 1.1 1992/05/10 19:42:22 rz Exp rz $
\chapter{Multivariate Interpolation}
\label{Sparse:Interp:Chap}

The problem for multivariate polynomials is a bit more complex.
Assume we are given a \key{black box} ${\scr B}_P$ for a multivariate
polynomial $P(X_1, \ldots, X_n)$, where each variable in $P$ appears
to degree less than $d$ and $P$ consists of no more than $T$ non-zero
terms.  \sectref{Interp:MDense:Sec} demonstrates how recursive use of
the univariate interpolation schemes gives a deterministic
multivariate algorithm that takes time $O(d^n)$.  For dense
polynomials ($T = O(n^d)$) this is the most efficient algorithm.

\sectref{Interp:PSparse:Sec} gives a probabilistic algorithm that
takes $O(T^3)$ time to compute $P$ from its values.  For practical
implementations this appears to be the most efficient interpolation
algorithm for sparse polynomials.  In \sectref{Interp:ZSparse:Sec} we
demonstrate how this algorithm can be made deterministic.

\sectref{Interp:BenOr:Sec} presents a different interpolation scheme,
based on the \key{Berlekamp-Massey algorithm} discussed in
\sectref{FPS:BM:Sec}.  Unlike the algorithm of
\sectref{Interp:ZSparse:Sec}, this algorithm does not require a bound
on the degrees of the variables in $P$.  Theoretically, it has better
performance than the algorithm of \sectref{Interp:ZSparse:Sec}, but from a practical
point of view the probabilistic approach is preferable.


\section{Multivariate Dense Interpolation}
\label{Interp:MDense:Sec}

As pointed out in the previous section, a polynomial in one variable
of degree $d$ can be determined from its values at $d+1$ points using
$O(d^2)$ arithmetic operations.  This result can be extended to
multivariate problems recursively.

Assume we are given a black box ${\scr B}_P$ for the polynomial
$P(X_1, \ldots, X_n)$ with a degree bound, $\deg_{X_i} P = d$.  
We can assume that $P$ has the form
\[
P(X_1, \ldots, X_n) = P_0(X_2, \ldots, X_n) X_1^d + 
\cdots + P_d(X_2, \ldots, X_n).
\]
Using the univariate interpolation schemes of the previous chapter
we can create black boxes ${\scr B}_{P_0}, \ldots, {\scr B}_{P_d}$, for
the $n-1$-variate coefficients of $P$.

To find the value of ${\scr B}_{P_i}(x_{20}, \ldots, x_{n0})$, choose
a sequence of $d+1$ different evaluations, points where the first
component is chosen at random and the last $n-1$ components are
$x_{20}, \ldots, x_{n0}$.  Applying the interpolation algorithms of
the last chapter, we can compute $P(X_1, x_{20}, \ldots, x_{n0})$:
\[
\left.
\begin{array}{c}
{\scr B}_P(x_{10}, x_{20}, \ldots, x_{n0}) \\ \vdots \\
{\scr B}_P(x_{1,d+1}, x_{20}, \ldots, x_{n0})
\end{array} \right\} \longrightarrow P(X_1, x_{20}, \ldots, x_{n0}).
\]
The coefficients of this polynomial are
\[
\begin{eqalign}
P(X_1, x_{20}, \ldots, x_{n0}) 
   & = P_0(x_2, \ldots, x_n) X_1^d + \cdots + P_d(x_2, \ldots, x_n), \\
   & = {\scr B}_{P_0}(x_2, \ldots, x_n) X_1^d + \cdots 
      + {\scr B}_{P_d}(x_2, \ldots, x_n).
\end{eqalign}
\]
So at the cost of $d+1$ evaluations, we can determine one value of
each of the $d+1$ black boxes, ${\scr B}_{P_0}, \ldots, {\scr
B}_{P_d}$. 

We now repeat this process, but with a different value for the $X_2$
coordinate.
\[
\left.
\begin{array}{c}
{\scr B}_P(x_{10}, x_{21}, \ldots, x_{n0}) \\ \vdots \\
{\scr B}_P(x_{1,d+1}, x_{21}, \ldots, x_{n0})
\end{array} \right\} \longrightarrow P(X_1, x_{21}, \ldots, x_{n0}),
\]
which gives another value for each of the black boxes.  Doing this
$d+1$ times, we can reconstruct the $P(X_1, X_2, x_{30}, \ldots,
x_{n0})$:
\small
\[
\hspace*{-14pt}\left.
\begin{array}{c}
\left.
\begin{array}{c}
{\scr B}_P(x_{10}, x_{20}, \ldots, x_{n0}) \\ \vdots \\
{\scr B}_P(x_{1,d+1}, x_{20}, \ldots, x_{n0})
\end{array} \right\} \rightarrow P(X_1, x_{20}, \ldots, x_{n0})\\
\vdots \\
\left.
\begin{array}{c}
{\scr B}_P(x_{10}, x_{2d}, \ldots, x_{n0}) \\ \vdots \\
{\scr B}_P(x_{1,d+1}, x_{2d}, \ldots, x_{n0})
\end{array} \right\} \rightarrow P(X_1, x_{2d}, \ldots, x_{n0})
\end{array} \right\}
\rightarrow P(X_1, X_2, x_{30}, \ldots, x_{n0})
\]
\normalsize

This recursive process can now be repeated with the $(d+1)^2$
coefficients $P(X_1, X_2, x_{30}, \ldots, x_{n0})$.  Each set of
coefficient values can be computed with $(d+1)^2$ values.  More
generally, the $(d+1)^k$ coefficients of $P(X_1, \ldots, X_k,
x_{k+1,0}, \ldots, x_{n0})$ can be computed with $(d+1)^k$ evaluations
of ${\scr B}_P$.  And thus $P(X_1, \ldots, X_n)$ can be determined
with $(d+1)^k$ values.

To analyze the complexity of this algorithm, let $I(d)$ denote the
time complexity of interpolating $d+1$ values to produce a univariate
polynomial of degree $d$.  And let $N_k$ be the complexity of the
previous algorithm for $k$ variables.  We have
\[
\begin{eqalign}
N_1 & = I(d), \\
N_2 & = (d + 1) N_1 + (d+1)I(d), \\
& \vdots \\
N_k & = (d+1) N_{k-1} + (d+1)^{k-1} I(d).
\end{eqalign}
\]
It is easy to show by induction that the solution to this recurrence
is
\[
N_k = k(d+1)^{k-1} I(d).
\]
Using a classical interpolation algorithm, $I(d) = O(d^2)$, so this
algorithm requires $O(n d^{n+1})$ operations.  Using the fast Fourier
transform reduces to $O(n d^n \log d)$.

\section{Probabilistic Sparse Interpolation}
\label{Interp:PSparse:Sec}

This section develops {\Zippel}'s sparse interpolation algorithm
\cite{Zippel90}, which gives a probabilistic resolution of the
interpolation problem.  This algorithm is given no information about
the number of non-zero terms in the polynomial being interpolated.
Instead it develops an estimate of the number of terms as each new
variable is introduced.  As a consequence its performance depends upon
the actual number of non-zero terms in the polynomial rather than an
{\em a priori} bound.  For most practical problems, this appears to be
the best algorithm available and tends to be more useful than the
deterministic algorithms presented in the following sections \cite{Manocha91}.

This section has been divided into three subsections.  In the first we
give a demonstration of the algorithm and its benefits.  In the
subsequent subsections we give a more formal presentation of its
details, and analyze the algorithm's performance.

\subsection{Heuristic Presentation}

As before, we wish to determine the polynomial $P(\vec X) \in L[\vec X]$
from its values, where $L$ is a field with sufficiently many distinct
elements.  We assume that $d_i$ bounds the degree of $X_i$ in $P$.  The
sparse interpolation algorithm computes $P$ one variable at a time.  That
is, we initially compute $P(a_1, a_2, \ldots, a_n)$, then $P(X_1, a_2,
\ldots,a_n)$, then $P(X_1, X_2, a_3, \ldots, a_n)$ and so on, until we have
determined $P(\vec X)$.  The introduction of each new variable is called a
{\em stage} of the algorithm.  We use clues from the polynomial produced in
the preceding stage to minimize the effort required to produce the next
polynomial in the sequence.

The description of the sparse interpolation algorithm becomes rather
involved and it is easy to get bogged down by all the subscripts and
variables involved, but it is fundamentally quite simple.  Here we
give an explicit example.

Let ${\scr B}_P$ be a black box representing the polynomial
\[
P(X, Y, Z) = 
  X^5 Z^2 + X^5 Z + X Y^4 + X Y Z^5 + Y^5 Z.
\]
We are given a degree bound of $5$ for each variable, so there are
$(5+1)^3 = 216$ different coefficients that need to be determined.
Given no other information, any deterministic interpolation scheme
would require $216$ calls to ${\scr B}_P$.  

By using a univariate interpolation scheme, either
\sectref{Interp:Lagrange:Sec} or \sectref{Interp:CRA:Sec}, we can
interpolate the values
\[
{\scr B}_P(x_0, y_0, z_0), {\scr b}_P(x_1, y_0, z_0), \ldots, 
{\scr B}_P(x_5, y_0, z_0).
\]
to produce the polynomial:
\[
P(X, y_0, z_0) = c_0 X^5 + c_1 X + c_2.
\]
Having introduced the first variable, we have completed the first
stage.

The purpose of the second stage is to produce $P(X, Y, z_0)$.  The
dense interpolation scheme repeats the process of the previous
paragraph, to produce $P(X, y_1, z_0)$.  The key step in the sparse
interpolation scheme is the recognition that $P(X,y_1, z_0)$ probably
only has $3$ non-zero coefficients and most likely has the form
\[
P(X, y_1, z_0) = c_3 X^5 + c_4 X + c_5.
\]
Since there are only three unknown coefficients in $P(X, y_1, z_0)$,
only $3$ different values of ${\scr B}_P$ are needed to determine
them.  Thus the overall computation of $P(X, Y, z_0)$ looks like:
\[
\hspace*{-1pt}\left.\begin{array}{c}
\left.\begin{array}{c}
{\scr B}_P(x_0, y_0, z_0) \\ \vdots \\ {\scr B}_P(x_5, y_0, z_0)
\end{array}\right\} \rightarrow c_0X^5 + c_1 X + c_2 \\
\left.\begin{array}{c}
{\scr B}_P(x_0, y_1, z_0) \\ {\scr B}_P(x_1, y_1, z_0) \\ 
{\scr B}_P(x_2, y_1, z_0)
\end{array}\right\} \rightarrow c_3X^5 + c_4 X + c_5 \\
\vdots \\
\left.\begin{array}{c}
{\scr B}_P(x_0, y_5, z_0) \\ {\scr B}_P(x_1, y_5, z_0) \\ 
{\scr B}_P(x_2, y_5, z_0)
\end{array}\right\} \rightarrow c_{15}X^5 + c_{16} X + c_{17} 
\end{array}\right\} \rightarrow d_0 X^5 + (d_1 Y^4 + d_2 Y) X + d_3 Y^5
\]
After the $6$ different univariate polynomials, $P(X, y_i, z_0)$ are
computed, their coefficients are interpolated to produce $P(X, Y,
z_0)$.  Notice that only $6 + 5 \times 3 = 21$ evaluations were
needed, while the dense interpolation scheme would require $36$.

The third stage proceeds in just the same fashion, but with even
greater savings.  We want to compute the six polynomials
\[
P(X, Y, z_i) = d_{4i} X^5 Z + d_{4i+1} X Y^4 + d_{4i+2} X Y + d_{4i+3} Y^5,
\]
for $i = 0, \ldots, 5$.  $P(X, Y, z_0)$ is known from the second
stage, and we assume that all the of $P(X, Y, z_i)$ have the same skeleton
as $P(X, Y, z_0)$.

Since there are only four unknowns in $P(X, Y, z_i)$, each set of the
$d_i$'s can be determined using only four values from ${\scr B}_P$ and
solving the resulting system of linear equations.  
These equations can be solved in $O(N^3)$ time using classical
Gaussian elimination.\footnote{Or, $O(N^{\omega})$, $\omega > 2$ using newer
optimized algorithms that aren't practical for problems of this size.}
Notice that this approach places absolutely no restrictions on the
values used for the interpolation points.  This can be quite useful in
certain problems.

If however, we are free to choose the evaluation points we can improve
this situation dramatically.  By choosing the values for $X$ and $Y$
appropriately, we can make the system of equations be a transposed
Vandermonde system, which can be solved efficiently.  Consider, for
instance, the computation of $P(X,Y,z_1)$.  For randomly chosen
$x_{\alpha}$ and $y_{\alpha}$ compute the four values
\[
\{P(1,1,z_1), P(x_{\alpha},y_{\alpha},z_1),
  P(x_{\alpha}^2,y_{\alpha}^2,z_1),
  P(x_{\alpha}^3,y_{\alpha}^3,z_1) \}.
\]
This gives the following \key{Vandermonde system of equations} for $d_4$,
$d_5$, $d_6$ and $d_7$:
\[
\begin{eqalign}
d_4 (x_{\alpha}^5)^0 + d_5 (x_{\alpha} y_{\alpha}^4)^0 +
  d_6 (x_{\alpha} y_{\alpha})^0 + d_7 (y_{\alpha}^5)^0
    & = {\scr B}_P(1,1,z_1), \\
d_4 (x_{\alpha}^5)^1 + d_5 (x_{\alpha} y_{\alpha}^4)^1 +
  d_6 (x_{\alpha} y_{\alpha})^1 + d_7 (y_{\alpha}^5)^1
    & = {\scr B}_P(x_{\alpha},y_{\alpha},z_1), \\
d_4 (x_{\alpha}^5)^2 + d_5 (x_{\alpha} y_{\alpha}^4)^2 +
  d_6 (x_{\alpha} y_{\alpha})^2 + d_7 (y_{\alpha}^5)^2
    & = {\scr B}_P(x_{\alpha}^2,y_{\alpha}^2,z_1), \\
d_4 (x_{\alpha}^5)^3 + d_5 (x_{\alpha} y_{\alpha}^4)^3 +
  d_6 (x_{\alpha} y_{\alpha})^3 + d_7 (y_{\alpha}^5)^3
    & = {\scr B}_P(x_{\alpha}^3,y_{\alpha}^3,z_1).
\end{eqalign}
\]

To compute $P(X, Y, z_1)$ we only need $4$ values from ${\scr B}_P$,
rather than the $36$ values that a dense interpolation scheme would
require.  $P(X, Y, z_2)$ through $P(X, Y, z_5)$ can be computed in the
same fashion.  Finally, the coefficients of these polynomials can be
interpolated using a univariate interpolation schemes to get a $P(X, Y, Z)$.
The total number of evaluations performed is $6 + 5 \times 3 + 5
\times 4 = 41$, which is a substantial improvement over the $216$
evaluations required by the dense interpolation.

For interpolation problems with more variables, this same recursive
structure is repeated. 

\subsection{Formal Presentation}

To fix our notation, assume we want to determine a sparse polynomial
$P(X_1, \ldots, X_n) \in L[\vec X]$ where we know that each $X_i$ does
not appear to degree higher than $d$ and that there are no more than
$T$ non-zero monomials in $P$, \ie
\[
P(X_1, \ldots, X_n) = c_1 \vec{X}^{\vec{e}_1} + c_2 \vec{X}^{\vec{e}_2} +
\cdots + c_T \vec{X}^{\vec{e}_T}.
\]
We assume that we are given a black box ${\scr B}_P$ that will return
$P(\vec{x})$ when given a vector of values $\vec{x}$.  The set of exponents
of a polynomial is called its \keyi{skeleton}, \ie{}
\[
\skel P = \{\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_T \}.
\]
We denote the {\em restriction of the skeleton} of $P$ to $X_1, \ldots,
X_k$, by $\skel_k P$ defined by 
\[
\skel_k P = \{\, (e_1, \ldots, e_k) \mid \exists \vec{e}\in \skel P \,.\,
   \vec{e} = (e_1, e_2, \ldots, e_k, \ldots)\,\}
\]
So $\skel_k P$ is the set of tuples consisting of the first $k$ components
of elements of $\skel P$.  Notice that for almost all  $x_{k+1}, \ldots,
x_n \in k$, 
\[
\skel P(X_1, \ldots, X_k, x_{k+1}, \ldots, x_n) 
\subseteq \skel_k P(X_1, \ldots, X_n).
\]
Equality almost always occurs, but it is not necessary.  For instance, let
$P(X_1, X_2)$ be the polynomial $X_1^2 + X_1 X_2 + X_1$.  Then we have the following
\[
\begin{eqalign}
\skel P(X_1, X_2) &= \{(2, 0), (1, 1) (1, 0)\}, \\
\skel_1 P(X_1, X_2) & = \{ (2), (1) \}, \\
\skel P(X_1, -1) & = \{ (2) \}.
\end{eqalign}
\]

We say that $(x_1, \ldots, x_v)$ is a \keyi{precise evaluation point}
if 
\[
\skel P(X_1, \ldots, X_k, x_{k+1}, \ldots, x_n) 
= \skel_k P(X_1, \ldots, X_n).
\]
for all $k$.  Otherwise, $(x_1, \ldots, x_v)$ is said to be an
\keyi{imprecise evaluation point}.  Using the results of the last
section we estimate the likelihood of imprecise evaluation points as
follows.

\begin{proposition}\label{Imprecise:Prob:Prop}
Let $f(X_1, \ldots, X_v)$ be a polynomial over an integral domain $A$.
Assume $\deg_{X_i} f \le D$ and $\terms F = T$.  Let ${\scr S}$ be a
subset of $A$ of cardinality $B$.  Then the probability that $(x_1,
\ldots, x_v)$ is an imprecise evaluation point of $f$, $x_i \in S$ is
bounded above by
\[
\frac{v(v-1)D T}{B}.
\]
\end{proposition}

\begin{proof}
For each $k$ we can write
\[
P(X_1, \ldots, X_v) = c_{1k}(X_{k+1}, \ldots, X_v) \vec{X}^{\vec{f}_{1k}}
+ \cdots +
c_{Tk}(X_{k+1}, \ldots, X_v) \vec{X}^{\vec{f}_{Tk}},
\]
where $f_{ik} \in \skel_k P$.  In order for $\vec{x}$ to be
an imprecise evaluation point, it must be the zero of one of the
$c_{ij}$.  We can add up the likelihood's variable by variable.  By
\longpropref{Zero:MPoly:Prop}, the probability that $\vec{x}$ will be
a zero of one of $c_{1k}, \ldots, c_{Tk}$ is no more than
\[
\frac{(v-k)DT}{B}.
\]
Summing these probabilities gives
\[
\frac{(v-1)DT}{B} + \frac{(v-2)DT}{B} + \cdots + \frac{DT}{B} =
\frac{v(v-1)DT}{B},
\]
as was to be shown. 
\end{proof}

\medskip

It is convenient to consider just one stage of the interpolation.
Assume we have performed the first $k-1$ stages of the sparse modular
algorithm and we are about to begin the $k$\th{} stage.  Throughout
$k${\th} stage, the values assigned to $X_{k+1}, \ldots, X_{n}$ will
not vary.  To simplify the notation, we omit them\footnote{In an
implementation this may be more than notational.  Eliminating the
variables that do not vary at this stage can save significant time
when computing the values of $P$.} and write
\[
P'(x_0, \ldots, x_k) = P(x_1, \ldots, x_k, x_{k+1,0}, \ldots, x_{n0}).
\]
From the $k${\th} stage's computation we have
\[
P(X_1, \ldots, X_{k-1}, x_{k0}, \ldots, x_{n0}) = p_{10} {\vec X}^{\vec e_1} +
p_{20} {\vec X}^{\vec e_2} + \cdots + p_{T0} {\vec X}^{\vec e_T}.
\]
We now assume that $\skel_k P = \{\vec{e}_1, \ldots, \vec{e}_T\}$,
\ie, every exponent combination of $X_1, \ldots, X_k$ that appears in
$P(X_1, \ldots, X_n)$ appears in $P'(X)$.  This is the probabilistic
assumption that underlies this algorithm.

The computation of $P(X_1, \ldots, X_{k-1}, X_k)$ proceeds in two phases.
In the first we determine  
\[
P'(X_1, \ldots, X_{k-1}, x_{kj}) = 
p_{1j} {\vec X}^{\vec e_1} + p_{2j} {\vec X}^{\vec e_2} + \cdots + p_{Tj} {\vec
X}^{\vec e_T},
\]
for $j = 1, \ldots, n$ by the following technique:

For randomly chosen $x_{kj}$ perform the following.  Pick a random
$k-1$-tuple denoted by $(y_1, \ldots, y_{k-1}) = \vec y$, such that
each of ${\vec y}^{\vec e_i}$ are distinct. Since the ${\vec e_i}$ are
known, verifying that this is the case is easy.  This value $\vec y$
allows us to set up the following (non-singular) transposed
\key{Vandermonde system} of linear equations
\begin{equation}
 \label{SPMod:Vandermonde:System:Eq}
\begin{eqalign}
  P'(1, \ldots, 1, x_{k,1})
    &= p_{1j} + p_{2j} + \cdots + p_{Tj}, \\
  P'(y_1, \ldots, y_{k-1}, x_{kj})
    &= p_{1j} {\vec y}^{\vec e_1} + p_{2j} {\vec y}^{\vec e_2} + \cdots +
      p_{Tj} {\vec y}^{\vec e_T}, \\
  P'(y_1^2, \ldots, y_{k-1}^2, x_{kj})
    &= p_{1j} {\vec y}^{2\vec e_1} + p_{2j} {\vec y}^{2\vec e_2} + \cdots +
      p_{Tj} {\vec y}^{2 \vec e_T}, \\
    &\vdots\\
  P'(y_1^T, \ldots, y_{k-1}^T, x_{kj})
    &= p_{1j} {\vec y}^{T \vec e_1} + p_{2j} {\vec y}^{T \vec e_2} + \cdots
      + p_{Tj} {\vec y}^{T \vec e_T},
\end{eqalign}
\end{equation}
which can be solved by the techniques of \sectref{Vandermonde:Sec} in
$O(T^2)$ time and $O(T)$ space.  The result is a polynomial
\[
P'(X_1, \ldots, X_{k-1}, x_{kj}),
\]
for each of $d$ values $x_{kj}$. 

In the second phase, we independently interpolate the coefficients of each
monomial, using the dense interpolation algorithm.  The results of these
interpolations can be combined to produce
\[
P'(X_1, \ldots, X_{k-1}, X_k) 
  = p_1(X_k) {\vec X}^{\vec e_1} + p_2(X_k) {\vec X}^{\vec e_2} 
     + \cdots + p_T(X_k) {\vec X}^{\vec e_T}.
\]
The dense interpolation, yielded the univariate polynomials $p_i(X_k)$.
This polynomial is in turn expanded to get
\[
P(X_1, \ldots, X_{k}, x_{k+1,0},\ldots, x_{n0}) 
  = p_{10}' {\vec X}^{\vec e_1'} + p_{20}' {\vec X}^{\vec e_2'} 
    + \cdots + p_{T0}' {\vec X}^{\vec e_T'},
\]
and we are ready to being the lift the next variable. 

\begindsacode
SI\=Stage ($P_{k-1}$, ${\scr B}_P$, $k$, $D$) := $\{$ \\
\> $S \leftarrow \skel P$; \\
\> $\ell \leftarrow \mbox{length}(S)$; \\
\> $\vec{y} \leftarrow \mbox{random}(F^{k-1})$; \\
\> $m[\cdot] \leftarrow \vec{y}^S$; \\
\> for \=$1 \le i \le D$ $\{$ \\
\>\> $x[i] \leftarrow \mbox{random}(F)$; \\
\>\> $B \leftarrow \{{\scr B}_P(\vec{y}^0, x[i]), \ldots, 
  {\scr B}_P(\vec{y}^{\ell}, x[i])\}$; \\
\>\> $L[i, \cdot] \leftarrow \mbox{SolveVandermondT}(x[\cdot], B[\cdot])$;\\
\>\> $\}$ \\
\> $P_k \leftarrow 0$; \\
\> for \=$i \le j < \ell$ $\{$\\
\> $P_k \leftarrow p_k + X^{S[j]} \cdot \mbox{NewtonInterp}(L[\cdot,
j], x[\cdot])$;\\
\>\> $\}$ \\
\> $\mbox{return}(P_k)$; \\
\> $\}$
\enddsacode

The key issue in this approach is guaranteeing that the
$\vec{y}^{\vec{e}_i} = m_i$ are distinct so that the Vandermonde
system is non-singular.  If the coefficient domain, $F$, is a unique
factorization domain we can do this easily.  For instance, assume $R$
is the rational integers.  We choose the components of $\vec p$ to be
distinct primes, viz., $\vec p = (2, 3, 5, \ldots)$.  By unique
factorization each of the $m_i$ will be distinct.

If the coefficient domain is a finite field $\F_q$, then the problem
can be more difficult.  The finite field must be contain at least
$\ell$ elements for the Vandermonde system to be non-singular.  For
the dense interpolation technique being discussed, the maximum value
of $\ell$ is $(d+1)^n$.  When $q < (d+1)^n$ the modular interpolation
technique can still be used but elements should be chosen from an
algebraic extension of $\F_q$ that has more than $(d+1)^n$ elements.

If the characteristic of $\F_q$ is sufficiently large, we can do better.  
Choose the components of $\vec p$ to be the rational primes, $(2, 3, 5,
\ldots)$.  If each of the $m_i$, when computed in $\Z$, is less than the
characteristic of $\F_q$ then they will be distinct as elements of
$\F_q$.  For this to be the case the characteristic must be greater than
\[
(2 \cdot 3 \cdots p_n)^d \approx n^{c_1 nd},
\]
by \propref{Prime:Product:Est:Prop}.

In the following paragraphs we analyze the hard case: We assume that $q$
is greater than $(d+1)^n$, but that the characteristic of $\F_q$ is less
than $n^{c_1 nd}$.  The actual analysis is straightforward but somewhat
lengthy.  We consider the following somewhat more general question since
its solution will be of use analyzing the sparse algorithms.  Let
$\{\vec e_i\}$ be a set of $T$ $n$-tuples where each component is
bounded by $d$.  (In the current case $T = (d+1)^n$.) What is the
probability that for a randomly chosen $\vec x \in \F_q^n$ there is an
$\vec e_i$ and $\vec e_j$ such that $\vec x^{\vec e_i} = \vec x^{\vec
e_j}$?

\medskip
We begin with an enumeration proposition.

\begin{proposition}
Let $\vec a$ be a fixed $n$-tuple where each component is
an element of $\Z/(m)$, $c$ be the common GCD of the $a_i$ and
$m$.  Let $\vec x$ be an $n$-tuple whose components range over $\Z_m$.  Then
$\vec a \cdot \vec x$ takes on $m/c$ distinct values.  These values
divide the different $\vec x$ into $m/c$ classes each containing $c
m^{n-1}$ different $\vec x$.
\end{proposition}

\begin{proof}
First we reduce to the case where $c = 1$.  Since $\vec a \cdot \vec x$ is
a multiple of $c$ for every $\vec x$, $\vec a \cdot \vec x$ can take on no more
than $m/c$ values, \ie, $0, c, 2c, \ldots$.  Let $\alpha c$ be one of these
values.  Each solution of 
\begin{equation}
{\vec a \cdot \vec x \over c} \equiv \alpha \pmod{m/c} 
\label{Eq:c}
\end{equation}
gives rise to $c^n$ solutions of $\vec a \cdot \vec x \equiv c \alpha
\pmod{m}$.  Thus if we can show that \eqnref{Eq:c} has $(m/c)^{n-1}$
solutions, we are finished.  The rest of the proof proceeds via a
slightly complicated induction.

Consider the one dimensional case, $a x \equiv b \pmod{m}$.  Since $a$
and $m$ are relatively prime, there is exactly one value of $x$ that
satisfies the relation for every value of $b$, as required by the
proposition.

Now assume the proposition is true for all vectors $\vec a$ of
dimension less than $n$.  Let $\alpha$ be an arbitrary element of
$\Z/(m)$.  We want to show that $\vec a \cdot \vec x \equiv \alpha
\pmod{m}$ has $m^{n-1}$ zeroes.  Without loss of generality we can
assume that $a_1$ is not zero.  If $a_1$ and $m$ are relatively prime
then for every choice of $a_2, \ldots, a_n$ there is a unique $a_1$
that satisfies the relation.  Thus there are $m^{n-1}$ zeroes of the
relation as desired.

Assume that $a_1$ and $m$ have a GCD of $g$.  The relation has no
zeroes if $g$ does not divide $a_2 x_2 + \cdots a_n x_n - \alpha$.
Thus we consider the number of zeroes of
\[
a_2 x_2 + \cdots a_n x_n \equiv \alpha \pmod{g}.
\]
Notice that $a_2, \ldots, a_n$ cannot have a GCD dividing $g$.  Thus
this equation has $g^{n-2}$ zeroes modulo $g$.  Each is the image of
$(m/g)^{n-1}$ elements modulo $m$.  Thus there are $m^{n-1}/g$ choices
of $a_2, \ldots, a_n$.  Each one will give rise to $g$ choices for
$x_1$ giving the desired result.
\end{proof}

\begin{proposition}
\label{SPMod:Count:Solns:Prop}
Let $\vec a$, $m$ and $c$ be as in the previous proposition.  Then there
are $c m^{n-1}$ distinct solutions to $\vec a \cdot \vec x = 0 \pmod{m}$.
\end{proposition}

\begin{proof}
$0$ is always one of the values of $\vec a \cdot \vec x$ since $\vec
x$'s components could be all zeroes.
\end{proof}

This result can now be used to answer the question raised above.

\begin{proposition}
\label{Interp:8:Prop}
Let $\vec e_1, \ldots, \vec e_T$ be $n$-tuples where each component is less
than $d$.  There exists no more than
\[
d \cdot T \cdot (T-1) \cdot (q-1)^{n-1} \over 2
\]
$n$-tuples $\vec x$ with components in $F_q$ such that for some $i$
and $j$, $\vec{x}^{\vec{e}_i}$ and $\vec{x}^{\vec{e}_j}$ have the same
values.  Equivalently, for at least
\[
(q-1)^{n-1} \left(q - 1 - {d \cdot T \cdot (T-1) \over 2}\right)
\]
$n$-tuples $\vec{x}$, $\vec{x}^{\vec{e}_i}$ takes on distinct values.
\end{proposition}

\begin{proof}
Let $g$ be a generator of the multiplicative group $\F_q^{\ast}$.  Then
for each $n$-tuple $\vec X$ we can assign another $n$-tuple $\vec a$ such that
$X_i = g^{a_i}$, assuming no $X_i$ is zero.  The $a_i$ are elements of
$\Z/(q-1)$.  Two monomials $\vec X^{\vec e_i}$ and $\vec X^{\vec e_j}$ have the
same value when 
\[
\vec x^{\vec e_i} = g^{\vec a \cdot \vec e_i} = g^{\vec a \cdot \vec e_j}
\vec x^{\vec e_j}.
\]
That is, when $\vec a \cdot (\vec e_i - \vec e_j) = 0 \pmod{q-1}$.  By
\propref{SPMod:Count:Solns:Prop} there are $c (q -1)^{n-1}$ such zeroes,
where $c$ is the GCD of the elements of $\vec e_i - \vec e_j$ and $q-1$.
Since $c \le d$ there are at most $d (q -1)^{n-1}$ tuples $\vec x$ that
cause these two terms to take on the same value.

There are $T(T-1)/2$ distinct pairs of $\vec e_i$, so the maximum number of
$\vec x$ that cause a pair of $\vec x^{\vec e_i}$ to take on the same value is
\[
{d \cdot T \cdot (T-1) \cdot (q-1)^{n-1} \over 2}.
\]
\end{proof}


Since there are only $(q-1)^{n-1}$ possible $\vec x$ (ignoring those
with a zero component), we have the following proposition.

\begin{proposition}
\label{Vandermonde:Vec:Indep:Prop}
The probability that a randomly chosen $\vec x$ will cause two of the $\vec
x^{\vec e_i}$ to have the same value is
\[
{d \cdot T \cdot (T-1) \over 2 (q - 1)}.
\]
\end{proposition}

If we wish the probability of a collision to be less than $\epsilon$, then
for dense polynomials this means that 
\[
q > {(d+1)^{2v+1} \over 2 \epsilon}.
\]
This is actually quite impractical for polynomials with large numbers
of variables and high degree.  Fortunately, many problems are sparse,
\ie,  $T \ll (d+1)^v$, which gives much better results.  This is the
topic of the next section.

\subsection{Analysis}

To analyze the sparse interpolation algorithm, assume that we are given a
black box ${\scr B}_P$ representing a polynomial $P(X_1, \ldots, X_n)$
where the degree of each variable is bounded by $d$.  Furthermore, assume
that $P$ has no more than $t$ non-zero terms, but this parameter is not
provided to the algorithm.  We first analyze the algorithm assuming that
the probabilistic assumptions at each stage are true.

At the beginning of stage $k$, we have a $P_{k-1}$, a polynomial in $k-1$
variables, with no more than $t$ non-zero terms.  To introduce the $X_k$ we
compute $d$ additional polynomials, each of which has the same skeleton as
$P_{k-1}$.  In total, these $d$ polynomials will require $dt$ values from
${\scr B}_P$ and each can be computer using $O(t^2)$ time and $O(t)$ space.
The interpolation to produce $P_k$ consists of at most $t$ individual dense
interpolations, each of which requires interpolates $d+1$ points at a cost
of $O(d^2)$ time and $O(d)$ space.  Thus the time required by the $k$\th{}
stage is
\[
d \times O(t^2) + t \times O(d^2) = O(dt(d+t)).
\]
The complete interpolation of $P(X_1, \ldots, X_n)$ will require $n$ stages
and will thus require $O(ndt(d+1))$ time.

The probabilistic nature of this algorithm arises from two sources: the
probabilistic assumption at each stage and the possibility that the
\key{Vandermonde system} is singular.  Here we will analyze the
likelihood that this assumption is false.  We have assumed that
\begin{equation}
 \label{SPMod:Prob:Assump:Eq}
\skel_{k-1} P = \skel P(X_1, \ldots, X_{k-1}, x_{k0}, \ldots, x_{n0}).
\end{equation}
If we write $P$ as
\[
\begin{eqalign}
  P(\vec{X}) 
  = &P_{k1}(X_k, \ldots, X_n) \tuple{X_1, \ldots,X_{k-1}}^{\vec{e}_{k1}} \\
 & + \cdots +
     P_{kt}(X_k, \ldots, X_n) \tuple{X_1, \ldots,X_{k-1}}^{\vec{e}_{kt}},
\end{eqalign}
\]
then \eqnref{SPMod:Prob:Assump:Eq} will be true if none of $P_{k1},
\ldots, P_{kt}$ vanish at $\tuple{x_{k0}, \ldots, x_{n0}}$.  If the
$x_{i0}$ are chosen from a set of $B$ elements, then by
\propref{Zero:MPoly:Prop} the probability that any of these
polynomials will vanish is
\[
\frac{(n - k) d t}{B}.
\]
Assuming that the likelihood of \eqnref{SPMod:Prob:Assump:Eq} being false
is independent for each $k$, then the probability
\eqnref{SPMod:Prob:Assump:Eq}is true for all $k$ is
\[
1 - \sum_{k=0}^{n-1} \frac{(n - k) d t}{B} = 1 - \frac{n(n+1)dt}{2B}.
\]

\smallskip
Now consider the probability that the Vandermonde systems that are set up
are singular.  These systems are of rank at most $t$.  By
\propref{Vandermonde:Vec:Indep:Prop}, the probability that this system is
singular is
\[
{d \cdot t \cdot (t - 1) \over 2 (B -1)} \approx {d t^2 \over 2B}.
\]
At each stage there are $d$ such systems to be solved and there are $n-1$
stages in all, so the probability that one of them will fail is bounded by
\[
{ n d^2 t^2 \over B}.
\]

Thus we have the following proposition.

\begin{proposition}
Let $P$ be a polynomial in $n$ variables, each of degree no more than $d$
and with $t$ ($> n$) non-zero terms.  Assume the coefficients of $P$ lie in
a field $k$ with at least $B$ elements.  If the starting point for the
sparse interpolation algorithm algorithm is chosen $k$, or a subset of $k$
of at least $B$ elements, the probability that one of the probabilistic
assumptions will be be wrong is less than
\[
{n d^2 t^2 \over B}.
\]
The randomly chosen values must be chosen from a set of at least
\[
n d^2 t^2 \over \epsilon
\]
values for the probability of error to be less than $\epsilon$.
\end{proposition}

Since we cannot know the true number of non-zero terms of $P$ before beginning
the algorithm, the random values must be chosen from a set of
\[
{n d^2 T^2 \over \epsilon}
\]
points.

\section{Deterministic Sparse Interpolation with Degree Bounds}
\label{Interp:ZSparse:Sec}

Deterministic sparse interpolation algorithms do exist.  One set,
developed by {\Zippel} \cite{Zippel90} independently by {\Grigoriev},
{\Karpinski}, {\Singer} \cite{Grigoriev90} is a derivative of
algorithms described in the previous section.  This technique applies
to all polynomials over a field of any characteristic but only solves
the problem of interpolation with degree bounds.  The second
technique, introduced by {\BenOr} and {\Tiwari} uses completely
different techniques and is only applicable to polynomials over the
integers.  However, this technique does not require a degree bound.

Using either of the deterministic solutions of the zero avoidance
problem given in the previous section (Propositions
\ref{Interp:13:Prop} and \ref{Interp:15:Prop}), it is possible to
modify the probabilistic sparse polynomial interpolation algorithm of
\sectref{Interp:PSparse:Sec} to make it deterministic.

As usual, we wish to interpolate a sparse polynomial with no more than
$T$ non-zero terms, $P(\vec X) \in L[X_1, \ldots, X_n]$, from its
values.  As in the last section we will only consider the case when
$F$ is the rational integers or a finite field of sufficiently large
characteristic.  For simplicity our discussion will use $F = \Z$.
Thus we can guarantee that the Vandermonde systems of equations are
always non-singular, by using as the initial starting point: $(2, 3,
5, \ldots, p_n)$, where $p_n$ is the $n$\th{} prime.

The only remaining source of erroneous answers in the probabilistic
algorithm of \sectref{Interp:PSparse:Sec}, is that coefficient polynomials
may vanish at the starting point.  To be more precise, assume the starting
point of the interpolation is $x_{10}, x_{20}, \ldots, x_{n0}$.  Consider
stage $k$, where we are introducing $X_k$.  We can write $P(\vec X)$ as
\[
\begin{eqalign}
P(\vec X) = &p_{1k}(X_{k+1}, \ldots, X_n) (X_1, \ldots, X_{k})^{\vec e_1} 
 + \cdots \\
&\qquad + p_{tk}(X_{k+1}, \ldots, X_n) (X_1, \ldots, X_{k})^{\vec
e_t}.
\end{eqalign}
\]
If the polynomials $p_{ik}$ do not vanish at the starting point, then
the skeleton produced at stage $k$ will be a correct image of $\skel P$.  If
this is the case we say the starting point is a {\em stage $k$ good
starting point\/}.  If the starting point was not good, then the resulting
skeleton will be strictly smaller than the correct one at that stage.

The deterministic version of the sparse modular algorithm assumes that
at stage $k-1$, the polynomial it is given has the correct skeleton.
It then produces a $k$ variable polynomial that has the correct
skeleton, by ensuring that it has used a starting point for which none
of $p_{ik}$ vanish.  This is easily done by performing the operations
of stage $k$, $T$ times, using $(x_{k+1,0}^s, \ldots, x_{n0}^s)$ as
the values for the undetermined variables.  Since the total number of
terms in $p_{ik}$ not greater than $T$, by \longpropref{Zero:Mon:Prop},
one of these starting points will be stage $k$ good.  Since we know
the correct $k-1$ skeleton it is not necessary to repeat lower stages
of the algorithm.

Thus this algorithm will require $T$ times more operations than the
probabilistic version.  The components of the evaluation points are always
primes (or a random integer for $X_n$).  Thus the largest component will be
$p_n^T$, whose size is approximately $O(T \log n \log \log n)$.

\section{Deterministic Sparse Interpolation without Degree Bounds}
\label{Interp:BenOr:Sec}

For simplicity assume that the polynomial being interpolated, $P(\vec
X)$ is an element of $\Z[\vec X]$ where $\Z$ is the rational integers.
For the case of polynomials over finite fields
\longpropref{Zero:Mon:Prop} and the following discussion apply, as was
in the case of the zero avoidance problem.  Unlike the probabilistic
algorithm given earlier, the only parameters we are given about
$P(\vec X)$ is the bound on the number of non-zero terms ($T$), and
the number of variables ($n$).

We assume that we are given a black box ${\scr B}_P$ for $P(\vec{X})$
and that $P$ has at most $T$ non-zero terms.  We write $P$ as
\[
P(\vec X) = c_1 {\vec X}^{\vec e_1} + c_2 {\vec X}^{\vec e_2} + \cdots +
c_T {\vec X}^{\vec e_T}.
\]
We will continue to let $d$ be the maximum degree to which any
$X_i$ appears in $P$, but it will only be used in the analysis.  It
will not be used in the algorithm itself.

If we choose a distinct prime for each $X_i$ then the quantities
${\vec X}^{\vec e_i} = m_i$ will all be distinct by unique
factorization.  These are the only evaluation points used in the
algorithm.  Denote the value of $P$ at the point ${\vec p}^j$ by $v_j
= P({\vec p}^j)$.  Each of the the $v_j$ gives us a constraint among
the $c_i$ and the $m_i$, {\em viz\/},
\begin{equation}
c_1 m_1^j + c_2 m_2^j + \cdots + c_T m_T^j = v_j.
\label{Eq:d}
\end{equation}

{\BenOr} and {\Tiwari}'s interpolation algorithm is significantly
subtler than the probabilistic one presented earlier.  It proceeds in
three basic steps:

\begin{itemize}

\item Compute the actual number of non-zero terms in $P$, $t$.

\item Determine the values of the $m_i$, and then the $\vec e_i$.

\item Find the values of the $c_i$.

\end{itemize}

The most difficult step is the second, where a quite clever technique
is used to find the $m_i$.

\def\Step#1{\smallskip\noindent{\bf Step #1:}}

\Step{1}
Notice that the rank of the system of equations \eqnref{Eq:d} is
exactly $t$, the number of non-zero monomials in $P$.  Thus $t$ can be
computed by taking the first $T$ equations, corresponding to $v_0,
v_1, \ldots, v_{T-1}$ and computing their rank.  This requires
$O(T^3)$ steps.

\Step{2}
Rather than determining the $m_i$ directly, we first find a polynomial
whose zeroes are the $m_i$.  It is easier to determine the
coefficients of this polynomial than the $m_i$ directly.  The roots of
this polynomial are then found using a $p$-adic version of Newton's
iteration.

Let 
\[
\begin{eqalign}
  Q(Z) & = (Z - m_1) (Z - m_2) \cdots (Z - m_t)\\
    & = q_t Z^t + q_{t-1} Z^{t-1} + \cdots + q_0,
\end{eqalign}
\]
where $q_t = 1$.  Note that $Q(m_i) = 0$.  Consider the sum 
\[
\sum_{1 \le j \le t} c_j Q(m_j) =
  \sum _{0 \le i < t} \left(c_1 m_1^i + \cdots + c_t m_t^i\right)\,q_i.
\]
The left hand side of the equation is clearly zero; while on the right the
coefficient of each of the $q_i$ is one fo the $v_i$.  Thus
\[
0 = v_t + v_{t-1} q_{t-1} + v_{t-2} q_{t-2} + \cdots + v_0 q_0.
\]
We can get additional equations resumming
\[
\sum_{1 \le j \le t} c_j m_j^k Q(m_j) 
 = \sum_{0 \le i \le t} (c_1 m_1^{i+k} + \cdots + c_t m_t^{i+k}) q_i
\]
for $k = 0, 1, \ldots, t-1$.  We then get the Toeplitz system of linear
equations:
\[
\begin{eqalign}
  v_0 q_0 + v_1 q_1 + v_2 q_2 + \cdots + v_{t-1} q_{t-1} &= - v_t\\
  v_1 q_0 + v_2 q_1 + v_3 q_2 +\cdots + v_{t} q_{t-1} &= - v_{t+1}\\
  \vdots\\
  v_{t-1}q_0 + v_{t} q_1 + v_{t+1} q_2 + \cdots + v_{2t-2} q_{t-1} &= -
    v_{2t-1}
\end{eqalign}
\]

This system of equations is a Toeplitz system.\index{Toeplitz matrix}
In general it would be difficult to even show that it to be
non-singular, however we can factor it as follows.  Let $\bf V$ denote
the matrix
\[
\pmatrix{v_0 & v_1 & \cdots & v_{t-1} \cr
v_1 & v_2 & \cdots & v_t \cr
\vdots & & & \vdots\cr
v_{t-1} & v_t & \cdots & v_{2t-2}\cr}
\]
Then ${\bf V}$ is the product
\small
\[
\begin{eqalign}
\pmatrix{1 & 1 & \cdots & 1 \cr
m_1 & m_2 & \cdots & m_t \cr
\vdots & & & \vdots\cr
m_1^{t-1} & m_2^{t-1} & \cdots & m_t^{t-1} \cr}
\cdot
\pmatrix{c_1 & 0 & \cdots & 0\cr
0 & c_1 & \cdots & 0 \cr
\vdots & & & \vdots \cr
0 & 0 & \vdots & c_t\cr}
\cdot
\pmatrix{
1& m_1 & \cdots & m_1^{t-1}\cr
1& m_2 & \cdots & m_2^{t-1}\cr
\vdots& & & \vdots\cr
1& m_t  & \cdots & m_t^{t-1}\cr} \\
\qquad = \pmatrix{v_0 & v_1 & \cdots & v_{t-1} \cr
v_1 & v_2 & \cdots & v_t \cr
\vdots & & & \vdots\cr
v_{t-1} & v_t & \cdots & v_{2t-2}\cr} 
\cdot
\pmatrix{
c_1& c_2 m_1 & \cdots & c_t m_1^{t-1}\cr
c_1 & c_2 m_2 & \cdots & c_t m_2^{t-1}\cr
\vdots& & & \vdots\cr
c_1& c_2 m_t  & \cdots & c_t m_t^{t-1}\cr}
\end{eqalign}
\]
\normalsize

The $q_i$ are now easy to determine because one of the three matrices in the
product is diagonal and the other two are of Vandermonde type.  Thus the
determination of the $q_i$ can be made in $O(t^2)$ operations.

The $m_i$ can now be determined by finding the zeroes of $Q$.  Notice
that the roots are real, distinct and are integers, so they are quite
easy to find.  The following algorithm of {\Loos} \cite{Loos83} can be
used.  Choose a prime $\ell$ such that $Q(Z)$ is square-free when
considered as a polynomial over $\Z/(\ell)$.  Denote the $t$ roots of
this polynomial by $m_{i0}$.  Then each of the $t$ roots modulo $\ell$
can be lifted to a root modulo $\ell^{2^s}$ by \key{Newton's
iteration}:
\[
m_{i,k+1} - m_{ik} = - {Q(m_{ik}) \over Q'(m_{ik})} \pmod{\ell^{2^{k+1}}}.
\]
As written a factor of $\ell^{2^k}$ can be removed from the above
equation.  The roots of $Q(Z)$ can be found in $O(dn \log n t^3)$
steps, see \cite{BenOr88,Loos83} for details.

Once we know the $m_i$'s the $\vec e_i$ can be determined by factoring each of
the $m_i$.  This is not hard, because the only possible divisors of the $m_i$
are the first $n$ primes.  

\Step{3}
Knowing the $m_i$ we can now determine the $c_i$ by solving the
Vandermonde system comprised of the first $t$ equations of type
\eqnref{Eq:d}.

The time required by this algorithm is dominated by by either step 1,
$O(T^3)$, or step 2, $O(dn \log n t^3)$.

\medskip
This solution to the interpolation problem only works for polynomials over
the reals.  For polynomials with coefficients in a finite field, $F$, we
have no way of ensuring the $m_i$ are distinct.  If however, we have a
degree bound on the $X_i$ in $P(\vec X)$ and the characteristic of $F$ is
sufficiently large then we can use a slight variation of the ideas given
above.

Let $d$ bound the degree of $X_i$ in $P(\vec X)$, and assume that the
characteristic of $F$ is larger than 
\[
2 \cdot (p_1 \cdots p_n)^d,
\]
where $p_i$ are the prime numbers.  Then each of the $m_i$ will remain
distinct.  By the results of \sectref{Zero:Finite:Sec}, we see that
the characteristic must be larger than $2^{c_2 d n \log n}$ for some
constant $c_2$.  Nonetheless this is small enough that arithmetic
operations in $F$ will be polynomial in $n$ and $d$.  Furthermore, we
can continue to recover the $e_i$ from the $m_i$ by factoring $m_i$ as
elements of $\Z$.

{\em See also \cite{Roth91}}

\section{Rational Function Interpolation}
\label{Sparse:Ratfun:Sec}

\section*{Notes}

\footnotesize

\notesectref{Interp:MDense:Sec} This approach was first suggested by
{\Brown} \cite{Brown:Euclid}.

\notesectref{Interp:BenOr:Sec}  {\Kaltofen} \cite{Kaltofen90c} has shown
that you ust a modular version of the {\BenOr}-{\Tiwari} algorithm for
efficiency. 

\normalsize
